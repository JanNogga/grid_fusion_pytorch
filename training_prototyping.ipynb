{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acbf036",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Function\n",
    "from apex import amp\n",
    "from typing import Optional, Tuple, List, Union, Callable\n",
    "from grid_fusion_pytorch.dataset import PointCloudDataset, CustomCollate\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "from tqdm.auto import tqdm\n",
    "from pprint import pprint\n",
    "import segmentation_models_pytorch as smp\n",
    "from segmentation_models_pytorch.losses import FocalLoss\n",
    "\n",
    "import torch_scatter\n",
    "\n",
    "import sys\n",
    "\n",
    "from grid_fusion_pytorch.render import render_grids, batch_fuse_to_grid, get_chunks\n",
    "#from pt_svr.render import render_grids, batch_fuse_to_grid, get_chunks\n",
    "from grid_fusion_pytorch.model import RefineModel\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e9e490",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from https://github.com/Hsuxu/Loss_ToolBox-PyTorch/blob/master/LovaszSoftmax/lovasz_loss.py\n",
    "def lovasz_grad(gt_sorted):\n",
    "    \"\"\"\n",
    "    Computes gradient of the Lovasz extension w.r.t sorted errors\n",
    "    See Alg. 1 in paper\n",
    "    \"\"\"\n",
    "    p = len(gt_sorted)\n",
    "    gts = gt_sorted.sum()\n",
    "    intersection = gts - gt_sorted.float().cumsum(0)\n",
    "    union = gts + (1 - gt_sorted).float().cumsum(0)\n",
    "    jaccard = 1. - intersection / union\n",
    "    if p > 1:  # cover 1-pixel case\n",
    "        jaccard[1:p] = jaccard[1:p] - jaccard[0:-1]\n",
    "    return jaccard\n",
    "\n",
    "# adapted from https://github.com/RaduAlexandru/lattice_net/blob/5255be27706e91b23f6e13c68c8097178cda1c34/latticenet_py/lattice/lovasz_loss.py#L23\n",
    "class LovaszSoftmax(nn.Module):\n",
    "    def __init__(self, reduction='mean'):\n",
    "        super(LovaszSoftmax, self).__init__()\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def lovasz_softmax_flat(self, inputs, targets):\n",
    "        num_classes = inputs.shape[1]\n",
    "        losses = []\n",
    "        for c in range(num_classes):\n",
    "            target_c = (targets == c).float()\n",
    "            nr_pixels_gt_for_this_class=target_c.sum()\n",
    "            if nr_pixels_gt_for_this_class==0:\n",
    "                continue #as described in the paper, we skip the penalty for the classes that are not present in this sample\n",
    "            if num_classes == 1:\n",
    "                input_c = inputs[:, 0]\n",
    "            else:\n",
    "                input_c = inputs[:, c]\n",
    "            loss_c = (torch.autograd.Variable(target_c) - input_c).abs()\n",
    "            loss_c_sorted, loss_index = torch.sort(loss_c, 0, descending=True)\n",
    "            target_c_sorted = target_c[loss_index]\n",
    "            losses.append(torch.dot(loss_c_sorted, torch.autograd.Variable(lovasz_grad(target_c_sorted))))\n",
    "        losses = torch.stack(losses)\n",
    "\n",
    "        if self.reduction == 'none':\n",
    "            loss = losses\n",
    "        elif self.reduction == 'sum':\n",
    "            loss = losses.sum()\n",
    "        else:\n",
    "            loss = losses.mean()\n",
    "        return loss\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        inputs = F.softmax(inputs, dim=1)\n",
    "        losses = self.lovasz_softmax_flat(inputs, targets)\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a0e426",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap_17 = torch.tensor([[161.,203,242], # cracker_box\n",
    "                        [227,88,34], # sugar_box\n",
    "                        [247,167,0], # mustard_bottle\n",
    "                        [100,68,34], # potted_meat_can\n",
    "                        [243,195,0], # banana\n",
    "                        [137,44,22], # bleach_cleanser\n",
    "                        [219,210,0], # mug\n",
    "                        [179,68,108], # sponge\n",
    "                        [41,183,0], # spatula\n",
    "                        [96,78,151], # power_drill\n",
    "                        [249,147,120], # wood_block\n",
    "                        [0,103,166], # extra_large_clamp\n",
    "                        [230,143,172], # softball\n",
    "                        [0,136,85], # golf_ball\n",
    "                        [132,132,130], # dice\n",
    "                        [195,179,129], # toy_airplane\n",
    "                        [191,0,50]])/255. # red_box\n",
    "\n",
    "labels = ['cracker_box', 'sugar_box', 'mustard_bottle', 'potted_meat_can', 'banana', 'bleach_cleanser', 'mug', 'sponge', 'spatula', 'power_drill', 'wood_block', 'extra_large_clamp', 'softball', 'golf_ball', 'dice', 'toy_airplane', 'red_box']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c91c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N x 3 ray origins and N x 3 ray_dirs\n",
    "def show_rays(ray_origins, ray_dirs):\n",
    "    ax = plt.figure(figsize=(24, 16)).add_subplot(projection='3d')\n",
    "    ax.view_init(elev=25., azim=45.)\n",
    "    _ = ax.quiver(\n",
    "      ray_origins[..., 0].flatten(),\n",
    "      ray_origins[..., 1].flatten(),\n",
    "      ray_origins[..., 2].flatten(),\n",
    "      ray_dirs[..., 0].flatten(),\n",
    "      ray_dirs[..., 1].flatten(),\n",
    "      ray_dirs[..., 2].flatten(), length=0.1, normalize=True, lw=0.05)\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('z')\n",
    "    plt.show()\n",
    "\n",
    "# N x 3 points\n",
    "def show_points(points):\n",
    "    ax = plt.figure(figsize=(24, 16)).add_subplot(projection='3d')\n",
    "    ax.view_init(elev=25., azim=45.)\n",
    "    for i, p in enumerate(points):\n",
    "        _ = ax.scatter(p[:,0], p[:,1], p[:,2], s=0.1, c=i*np.ones(len(points[0])), cmap='tab20', vmin=0, vmax=len(points))\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('z')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d74f192",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE = 'normal'\n",
    "DATASET_NAME = 'overfit_O39_S1_C32_pcd_' + MODE\n",
    "NUM_CAMS = -1\n",
    "BATCH_SIZE = 1\n",
    "INSPECT_SHAPES = False\n",
    "RANDOM_PCD = False\n",
    "\n",
    "# create a dummy dataset and showcase dataloading\n",
    "test_dataset = PointCloudDataset(root='/home/nfs/inf6/data/datasets/semantic_pcd_data/'+DATASET_NAME, split='full',\n",
    "                                 num_steps=-1, num_cams=NUM_CAMS, random_pcd=RANDOM_PCD, cam_world=False)\n",
    "if INSPECT_SHAPES:\n",
    "    print('Length of test_dataset:', len(test_dataset), '\\n')\n",
    "\n",
    "# look at the data returned by this dataset\n",
    "if INSPECT_SHAPES:\n",
    "    test_output = test_dataset.__getitem__(0)\n",
    "    pcd, semseg, cam_pose, depth, cam_k, gt = test_output\n",
    "    print('Dataset __getitem__ output len/shapes:\\n', len(pcd), semseg.shape, cam_pose.shape, depth.shape, cam_k.shape, gt.shape, '\\n')\n",
    "    print('pcd element shapes:')\n",
    "    for item in pcd:\n",
    "        print(item.shape)\n",
    "    print()\n",
    "\n",
    "# define a collate function and a dataloader\n",
    "collate = CustomCollate(min_num_steps=1, max_num_steps=2)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate)\n",
    "# look at the output shapes of each minibatch\n",
    "if INSPECT_SHAPES:\n",
    "    for pcd, semseg, cam_pose, depth, cam_k, gt in test_dataloader:\n",
    "        print('Minibatch output len/shapes:\\n', len(pcd), semseg.shape, cam_pose.shape, depth.shape, cam_k.shape, gt.shape)\n",
    "        print('pcd batched element shapes:')\n",
    "        for item in pcd:\n",
    "            print(item.shape)\n",
    "    print()\n",
    "\n",
    "# example for adapting the maximum number of fusion/refinement steps\n",
    "if INSPECT_SHAPES:\n",
    "    test_dataloader.collate_fn.set_steps(1, 8)\n",
    "    for pcd, semseg, cam_pose, depth, cam_k, gt in test_dataloader:\n",
    "        print('Minibatch output len/shapes:\\n', len(pcd), semseg.shape, cam_pose.shape, depth.shape, cam_k.shape, gt.shape)\n",
    "        print('pcd batched element shapes:')\n",
    "        for item in pcd:\n",
    "            print(item.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e64b298",
   "metadata": {},
   "outputs": [],
   "source": [
    "### positional encoding taken from https://colab.research.google.com/drive/1TppdSsLz8uKoNwqJqDGg8se8BHQcvg_K?usp=sharing#scrollTo=rrbs7YoMHAbF\n",
    "class PositionalEncoder(nn.Module):\n",
    "  \"\"\"\n",
    "  Sine-cosine positional encoder for input points.\n",
    "  \"\"\"\n",
    "  def __init__(\n",
    "    self,\n",
    "    d_input: int,\n",
    "    n_freqs: int,\n",
    "    log_space: bool = False\n",
    "  ):\n",
    "    super().__init__()\n",
    "    self.d_input = d_input\n",
    "    self.n_freqs = n_freqs\n",
    "    self.log_space = log_space\n",
    "    self.d_output = d_input * (1 + 2 * self.n_freqs)\n",
    "    self.embed_fns = [lambda x: x]\n",
    "\n",
    "    # Define frequencies in either linear or log scale\n",
    "    if self.log_space:\n",
    "        freq_bands = 2.**torch.linspace(0., self.n_freqs - 1, self.n_freqs)\n",
    "    else:\n",
    "        freq_bands = torch.linspace(2.**0., 2.**(self.n_freqs - 1), self.n_freqs)\n",
    "\n",
    "    # Alternate sin and cos\n",
    "    for freq in freq_bands:\n",
    "        self.embed_fns.append(lambda x, freq=freq: torch.sin(x * freq))\n",
    "        self.embed_fns.append(lambda x, freq=freq: torch.cos(x * freq))\n",
    "  \n",
    "  def forward(self,x) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Apply positional encoding to input.\n",
    "    \"\"\"\n",
    "    return torch.concat([fn(x) for fn in self.embed_fns], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78291182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# voxel grid setup\n",
    "voxel_base_num = 100\n",
    "voxel_grid_config = {}\n",
    "voxel_grid_config['range_min'] = test_dataset.range_min\n",
    "voxel_grid_config['range_max'] = test_dataset.range_max\n",
    "axis_range = voxel_grid_config['range_max'][:3] - voxel_grid_config['range_min'][:3]\n",
    "axis_range /= torch.min(axis_range)\n",
    "voxel_grid_config['world_size'] =  (torch.ones(3)*voxel_base_num*axis_range).long() #[150,120,105] #[300,240,210]\n",
    "# modify so each dim is divisible by 4\n",
    "voxel_grid_config['world_size'] -= torch.remainder(voxel_grid_config['world_size'], 4)\n",
    "voxel_grid_config['channels'] = 39 #test_dataset.num_classes\n",
    "voxel_grid_config['voxel_size'] = (axis_range/voxel_grid_config['world_size'].float()).mean()\n",
    "voxel_grid_config['anchor'] = (voxel_grid_config['range_max'][:3] + voxel_grid_config['range_min'][:3]) / 2\n",
    "voxel_grid_config['density_factor'] = 10000.\n",
    "voxel_grid_config['t_near'] = 0.1\n",
    "voxel_grid_config['t_far'] = 2.0\n",
    "# use positional encoding\n",
    "voxel_grid_config['use_pos_enc'] = False\n",
    "voxel_grid_config['num_freqs'] = 8\n",
    "if voxel_grid_config['use_pos_enc']:\n",
    "    grid_encoder = PositionalEncoder(3, voxel_grid_config['num_freqs'])\n",
    "    with torch.no_grad():\n",
    "        linspaces = [torch.linspace(voxel_grid_config['range_min'][i], voxel_grid_config['range_max'][i], voxel_grid_config['world_size'][i]) for i in range(3)]\n",
    "        coords = torch.cartesian_prod(*linspaces).view(*voxel_grid_config['world_size'],3)\n",
    "        voxel_grid_config['pos_enc'] = grid_encoder(coords).permute(3,0,1,2)\n",
    "else:\n",
    "    voxel_grid_config['pos_enc'] = None\n",
    "    \n",
    "np.save('voxel_grid_config.npy', voxel_grid_config)\n",
    "\n",
    "print('Channels:', voxel_grid_config['channels'])\n",
    "print('World size:', voxel_grid_config['world_size'])\n",
    "print('Voxel size:', voxel_grid_config['voxel_size'])\n",
    "print('Voxel min:', voxel_grid_config['range_min'][:3])\n",
    "print('Voxel max:', voxel_grid_config['range_max'][:3])\n",
    "if voxel_grid_config['use_pos_enc']:\n",
    "    print('Positional encoding shape:', voxel_grid_config['pos_enc'].shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7c7480",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "dtype = torch.float32\n",
    "\n",
    "\n",
    "VERBOSE = False\n",
    "\n",
    "# renderer setup\n",
    "N_RAYS = 896 #4000\n",
    "N_POINTS =  192 #300\n",
    "N_EPOCHS = 5000\n",
    "\n",
    "# model setup\n",
    "TRAIN_MODEL = True\n",
    "\n",
    "#model = UNet3d(in_channels=1+voxel_grid_config['channels']+2, out_channels=1+voxel_grid_config['channels'], trilinear=False)\n",
    "#model = model.to(device)\n",
    "#model.train()\n",
    "\n",
    "num_ignore_channels = 1 if not voxel_grid_config['use_pos_enc'] else 1 + voxel_grid_config['pos_enc'].shape[0]\n",
    "\n",
    "model_kwargs = {'in_channels' : [1+voxel_grid_config['channels']+num_ignore_channels, 64, 128, 64],\n",
    "                'out_channels' : [64, 128, 64, 1+voxel_grid_config['channels']],\n",
    "                'mode' : 'hourglass', # 'resnet', 'hourglass'\n",
    "                'norm_layer': 'GroupNorm',\n",
    "                'non_lin': 'LeakyReLU',\n",
    "                'num_ignore_channels': num_ignore_channels} \n",
    "np.save('model_config.npy', model_kwargs)\n",
    "model = RefineModel(**model_kwargs)\n",
    "#model = ResNet3d(in_channels=[1+voxel_grid_config['channels']+2,64,64,64,64,64,64,64,64],\n",
    "#                 out_channels=[64,64,64,64,64,64,64,64,1+voxel_grid_config['channels']], normalize=False)\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "\n",
    "# training setup\n",
    "LR = 0.02\n",
    "\n",
    "WEIGHT_SEMSEG = 0.5\n",
    "criterion = torch.nn.CrossEntropyLoss() # FocalLoss(mode='multiclass') # torch.nn.CrossEntropyLoss()\n",
    "WEIGHT_DICE = 0.5\n",
    "criterion_dice = LovaszSoftmax() # FocalLoss(mode='multiclass') # torch.nn.CrossEntropyLoss()\n",
    "WEIGHT_BACKGROUND = 2. #100. #8e-1\n",
    "criterion_background = torch.nn.BCEWithLogitsLoss() #torch.nn.L1Loss() # torch.nn.HuberLoss(delta=0.01)\n",
    "WEIGHT_FOREGROUND = 0 # 1e-1\n",
    "criterion_foreground = torch.nn.L1Loss() # torch.nn.HuberLoss(delta=0.01)\n",
    "WEIGHT_DEPTH = 1. #1.\n",
    "criterion_depth = torch.nn.L1Loss()\n",
    "WEIGHT_DENSITY = 5e-4 # use 1e-1 to match, 5e-3 to regularize\n",
    "#criterion_density = lambda x: torch.mean(torch.norm(torch.flatten(x, start_dim=1), p=1, dim=-1))\n",
    "criterion_density = torch.nn.L1Loss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR)\n",
    "model, optimizer = amp.initialize(model, optimizer, opt_level='O2')\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=50)\n",
    "\n",
    "save_grid_batch = False\n",
    "if TRAIN_MODEL:\n",
    "    ##### train\n",
    "    best_loss = 1e25\n",
    "    test_dataloader.collate_fn.set_steps(1, 1)\n",
    "    progress_bar = tqdm(range(N_EPOCHS), total=N_EPOCHS, position=0, leave=True)\n",
    "    patience = 0\n",
    "    for epoch in progress_bar:\n",
    "        for pcd, semseg, cam_pose, depth, cam_k, gt in test_dataloader:\n",
    "            semseg, cam_pose, depth, cam_k = semseg.to(device), cam_pose.to(device), depth.to(device), cam_k.to(device)\n",
    "            if VERBOSE:\n",
    "                print('DATA LOADING')\n",
    "                print('Minibatch output len/shapes\\n', 'len(pcd), semseg.shape, cam_pose.shape, depth.shape, cam_k.shape')\n",
    "                print(len(pcd), semseg.shape, cam_pose.shape, depth.shape, cam_k.shape)\n",
    "                print()\n",
    "\n",
    "            ##### FUSION STEP\n",
    "            if VERBOSE:\n",
    "                print('FUSION STEP')\n",
    "            grid_batch = None\n",
    "            k = 0\n",
    "            for k, point_cloud_batch in enumerate(pcd):\n",
    "                if VERBOSE:\n",
    "                    print('Point cloud batch shape:', point_cloud_batch.shape)\n",
    "                # fuse sensor data to voxel logits\n",
    "                with torch.no_grad():\n",
    "                    grid_batch = batch_fuse_to_grid(point_cloud_batch.to(device), grid_batch,\n",
    "                                                    world_size=voxel_grid_config['world_size'],\n",
    "                                                    channels=voxel_grid_config['channels'],\n",
    "                                                    range_min=voxel_grid_config['range_min'],\n",
    "                                                    range_max=voxel_grid_config['range_max'],\n",
    "                                                    density_step=voxel_grid_config['density_factor']*voxel_grid_config['voxel_size'])        \n",
    "            # switch to probabilities\n",
    "            grid_batch_unrefined = torch.cat([grid_batch[:,:1], torch.exp(grid_batch[:,1:-1]), grid_batch[:,-1:]], dim=1)\n",
    "            # concat positional encoding if necessary\n",
    "            grid_batch_input = torch.cat([grid_batch_unrefined, \n",
    "                                          voxel_grid_config['pos_enc'].unsqueeze(0).expand(grid_batch_unrefined.shape[0],*voxel_grid_config['pos_enc'].shape).to(grid_batch_unrefined.device)],\n",
    "                                          dim=1) if voxel_grid_config['use_pos_enc'] else grid_batch_unrefined\n",
    "            # refine, switches to logits\n",
    "            grid_batch_refined = model(grid_batch_input)\n",
    "            # if necessary remove positional encoding again\n",
    "            if voxel_grid_config['use_pos_enc']:\n",
    "                grid_batch_refined = grid_batch_refined[:,:-voxel_grid_config['pos_enc'].shape[0]]\n",
    "            if VERBOSE:\n",
    "                print('Grid batch shape:', grid_batch.shape)\n",
    "                print()\n",
    "            # -> at this point grids is (BS, C, H, W, D)\n",
    "\n",
    "            ##### REFINE STEP\n",
    "            cam_pose_chunks, cam_k_chunks  = get_chunks(cam_pose, dim=1), get_chunks(cam_k, dim=1)\n",
    "            semseg_chunks, depth_chunks = get_chunks(semseg, dim=1), get_chunks(depth, dim=1)\n",
    "            loss, loss_semseg, loss_dice, loss_background, loss_foreground, loss_depth, loss_density = 0, 0, 0, 0, 0, 0, 0\n",
    "            total_chunk_size = cam_pose.shape[1]\n",
    "            for chunk_id in range(len(semseg_chunks)):\n",
    "                chunk_size = cam_pose_chunks[chunk_id].shape[1]\n",
    "                chunk_weight = chunk_size / total_chunk_size\n",
    "                render, render_depth, composite_mask, gt_labels, gt_depth = render_grids(grid_batch_refined, voxel_grid_config, \n",
    "                                                                            cam_pose_chunks[chunk_id], cam_k_chunks[chunk_id], n_rays=N_RAYS,\n",
    "                                                                            n_points=N_POINTS, semseg=semseg_chunks[chunk_id],\n",
    "                                                                            depth=depth_chunks[chunk_id], verbose=False, hierarchical=True,\n",
    "                                                                            downsample_density=True)\n",
    "                \n",
    "                # backprop loss between ray_gt and ray_marching\n",
    "                ray_mask = gt_labels != -1\n",
    "                #print(render[ray_mask.squeeze(-1)].shape, gt_labels[ray_mask].shape)\n",
    "                loss_semseg += chunk_weight * criterion(render[ray_mask.squeeze(-1)], gt_labels[ray_mask])\n",
    "                loss_dice += chunk_weight * criterion_dice(render[ray_mask.squeeze(-1)], gt_labels[ray_mask])\n",
    "                background_mask = torch.logical_not(ray_mask).squeeze(-1)\n",
    "                #print('composite mask:', composite_mask.shape)\n",
    "                #print('background mask:', background_mask.shape)\n",
    "                probs = torch.clamp(composite_mask,1e-3,1-1e-3)\n",
    "                logits = 0.5*(torch.log(probs) - torch.log(1 - probs) + 1)\n",
    "                loss_background += chunk_weight * criterion_background(logits, background_mask.float())\n",
    "                #loss_background += chunk_weight * criterion_background(composite_mask[background_mask], torch.ones_like(composite_mask[background_mask]))\n",
    "                loss_foreground += chunk_weight * criterion_foreground(composite_mask[ray_mask.squeeze(-1)], torch.zeros_like(composite_mask[ray_mask.squeeze(-1)]))\n",
    "                #print(ray_mask.shape, render.shape, composite_mask.shape)\n",
    "                if gt_depth is not None: \n",
    "                    loss_depth += chunk_weight * criterion_depth(render_depth[ray_mask.squeeze(-1)], gt_depth[ray_mask])\n",
    "                else:\n",
    "                    loss_depth += torch.zeros(1).to(semseg.device)\n",
    "                loss_density += chunk_weight * criterion_density(grid_batch_refined[:,0], grid_batch_unrefined[:,0]) # criterion_density(grid_batch_refined[:,0])\n",
    "\n",
    "            loss = WEIGHT_SEMSEG * loss_semseg + WEIGHT_DICE * loss_dice + WEIGHT_DEPTH * loss_depth  + WEIGHT_BACKGROUND * loss_background + WEIGHT_FOREGROUND * loss_foreground + WEIGHT_DENSITY*loss_density\n",
    "            optimizer.zero_grad()\n",
    "            with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step(loss.item())\n",
    "            if loss.item() < best_loss:\n",
    "                best_loss = loss.item()\n",
    "                torch.save(model.state_dict(), 'net_best_'+MODE+'.pt')\n",
    "                patience = 0\n",
    "            else:\n",
    "                patience += 1\n",
    "            # Show current training stats\n",
    "            episode_summary = [f\"{epoch+1}:\"] + [f'L: {loss.item():.3f}, Seg: {loss_semseg.item():.3f}, DI: {loss_dice.item():.3f}, Z: {loss_depth.item():.3f}, BG:{loss_background.item():.3f}, D:{loss_density.item():.3f}, FG:{loss_foreground.item():.3f}, Best: {best_loss:.3f}, P: {patience}']\n",
    "            # Set progress bar\n",
    "            progress_bar.set_description(\"\".join(episode_summary))\n",
    "        if patience > 200:\n",
    "            break\n",
    "        # z=0.033"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5621eeae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
